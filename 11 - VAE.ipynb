{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C60ZAm4-Yo4M"
   },
   "source": [
    "# Week 12 - Generative models  \n",
    "\n",
    "## Part 2: Variational Autoencoders (VAEs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsSfbUYyiOP2"
   },
   "source": [
    "This part we move towards a non-linear mapping between the latent space z and the observation/input space x. Concretely, we will implement a VAE in Pyro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCHBydp2Yo4N"
   },
   "source": [
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5377,
     "status": "ok",
     "timestamp": 1646648637408,
     "user": {
      "displayName": "Filipe Rodrigues",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgANv2Od365vLQDnVjoxw2PUote-kpb5nugTmcQwA=s64",
      "userId": "15636531912642599438"
     },
     "user_tz": -60
    },
    "id": "Ei1QbnmhYyXM",
    "outputId": "ff76128a-ac98-4741-e58e-00fb4a40241e"
   },
   "outputs": [],
   "source": [
    "# Install Pyro, if necessary\n",
    "#!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4897,
     "status": "ok",
     "timestamp": 1646648642167,
     "user": {
      "displayName": "Filipe Rodrigues",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgANv2Od365vLQDnVjoxw2PUote-kpb5nugTmcQwA=s64",
      "userId": "15636531912642599438"
     },
     "user_tz": -60
    },
    "id": "iGyKT_grYo4O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mnist_cached import MNISTCached as MNIST\n",
    "from mnist_cached import setup_data_loaders\n",
    "from vae_plots import mnist_test_tsne, plot_llk, plot_vae_samples\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)\n",
    "\n",
    "# matplotlib style options\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktkbYC6AYo4X"
   },
   "source": [
    "We will first use the same Iris data that we used for PPCA.\n",
    "\n",
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1646648642565,
     "user": {
      "displayName": "Filipe Rodrigues",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgANv2Od365vLQDnVjoxw2PUote-kpb5nugTmcQwA=s64",
      "userId": "15636531912642599438"
     },
     "user_tz": -60
    },
    "id": "ge0Jqp1xYo4Y",
    "outputId": "b14f96fc-eb68-48c4-d8d5-b8f2dc489270"
   },
   "outputs": [],
   "source": [
    "# load csv\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = torch.tensor(iris.data, dtype=torch.float32)\n",
    "y = iris.target\n",
    "\n",
    "# standardize data\n",
    "X = (X-X.mean(dim=0)) / X.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG-46wBlYo5d"
   },
   "source": [
    "## Variational Autoencoders in Pyro\n",
    "\n",
    "We will now implement a VAE in Pyro. Let us begin by recapping the model introduced in the class. The generative process can be summarize as follows:\n",
    "\n",
    "1. For each observation $n \\in \\{1,\\dots,N\\}$\n",
    "    1. Sample latent projection $\\textbf{z}_n \\sim \\mathcal{N}(\\textbf{z}_n|\\textbf{0}, \\textbf{I})$\n",
    "    2. Sample observation $\\textbf{x}_n \\sim \\mathcal{N}(\\textbf{x}_n|f_{\\boldsymbol\\theta}(\\textbf{z}_n))$\n",
    "\n",
    "Note that the neural network $f$ is outputting both the mean and variance of the Gaussian. \n",
    "\n",
    "Our goal is to compute the posterior distribution over the latent variables $\\textbf{z}_n$, while jointly finding point estimates for the parameters of that neural network $\\boldsymbol\\theta$. Note that you could also treat $\\boldsymbol\\theta$ as latent variables and perform inference on them as well (i.e. a Bayesian neural network), but, for this particular model, point estimates are usually sufficient. \n",
    "\n",
    "We will also need an approximate distribution $q(\\textbf{z})$ for VI. As we saw in the slides, we will use a second neural network $g$ to parameterize this variational distribution $q$. Therefore, we can represent the entire process of the training a VAE as follows:\n",
    "\n",
    "<img style=\"width:50%\" src=\"http://mlsm.man.dtu.dk/vae.png\">\n",
    "\n",
    "In the diagram, $g$ is the encoder network and $f$ is the decoder network. Lets begin by implementing those two neural networks in Torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the PyTorch module that parameterizes the\n",
    "# diagonal gaussian distribution q(z|x)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        super().__init__()\n",
    "        # setup the three linear transformations used\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define the forward computation on the image x\n",
    "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        # then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        # then return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale\n",
    "\n",
    "\n",
    "# define the PyTorch module that parameterizes the\n",
    "# observation likelihood p(x|z)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim, binary=False):\n",
    "        self.binary = binary\n",
    "        super().__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, output_dim)\n",
    "        if not self.binary:\n",
    "            self.fc22 = nn.Linear(hidden_dim, output_dim)\n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        # return the parameter for the output Bernoulli\n",
    "        # each is of size batch_size x input_dim\n",
    "        if self.binary:\n",
    "            out = torch.sigmoid(self.fc21(hidden))\n",
    "        else:\n",
    "            out = (self.fc21(hidden), self.softplus(self.fc22(hidden)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined $f$ and $g$, we can use them to implement the model() and guide() in Pyro:\n",
    "\n",
    "- The guide() will take the observations $\\textbf{x}$ as input, pass them through the encoder network $g$, and use the output of the encoder to parameterize the variational approximation $q(\\textbf{z})$. You can do a forward pass through the encoder using: ``self.encoder.forward(x)``. \n",
    "\n",
    "- The model() just follows the generative process of the VAE described above. For each observation $n$, it samples $\\textbf{z}_n \\sim \\mathcal{N}(\\textbf{z}_n|\\textbf{0}, \\textbf{I})$ and passes the samples through the decoder network $f$, in order to produce the parameters of the observation distributon $\\mathcal{N}(\\textbf{x}_n|f_{\\boldsymbol\\theta}(\\textbf{z}_n))$. You can do a forward pass through the decoder using: ``self.decoder.forward(z)``. \n",
    "\n",
    "Can try to you implement the guide() and model() for the Pyro model below? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a PyTorch module for the VAE\n",
    "class VAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=50, hidden_dim=400, input_dim=784, binary=False, use_cuda=False):\n",
    "        self.input_dim = input_dim\n",
    "        self.binary = binary\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = Encoder(z_dim, hidden_dim, input_dim)\n",
    "        self.decoder = Decoder(z_dim, hidden_dim, input_dim, binary=binary)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "    # define the model p(x|z)p(z)\n",
    "    def model(self, x):\n",
    "        # register PyTorch module `decoder` with Pyro\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc = torch.zeros(x.shape[0], self.z_dim)\n",
    "            z_scale = torch.ones(x.shape[0], self.z_dim)\n",
    "            z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            img = self.decoder.forward(z)\n",
    "            if self.binary:\n",
    "                # sample from the Bernoulli distribution\n",
    "                pyro.sample(\"obs\", dist.Bernoulli(img, validate_args=False).to_event(1), obs=x.reshape(-1, self.input_dim))\n",
    "            else:\n",
    "                # sample from the Gaussian distribution\n",
    "                loc, scale = img\n",
    "                pyro.sample(\"obs\", dist.Normal(loc, scale, validate_args=False).to_event(1), obs=x.reshape(-1, self.input_dim))\n",
    "\n",
    "    # define the guide (i.e. variational distribution) q(z|x)\n",
    "    def guide(self, x):\n",
    "        # register PyTorch module `encoder` with Pyro\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            z_loc, z_scale = self.encoder.forward(x)\n",
    "            z = pyro.sample(\"z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "\n",
    "\n",
    "    # define a helper function for reconstructing images (this will useful later - ignore for now)\n",
    "    def reconstruct_img(self, x):\n",
    "        # encode image x\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        # sample in latent space\n",
    "        z = dist.Normal(z_loc, z_scale).sample()\n",
    "        # decode the image (note we don't sample in image space)\n",
    "        loc_img = self.decoder(z)\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Pyro model is defined, lets run Bayesian inference on it. Note that the encoder and decoder networks will be trained jointly as part of the VI process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(z_dim=2, hidden_dim=20, input_dim=4, use_cuda=False)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": 1.0e-3}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "#elbo = Trace_ELBO()\n",
    "elbo = Trace_ELBO(num_particles=3)\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=elbo)\n",
    "\n",
    "train_elbo = {}\n",
    "test_elbo = {}\n",
    "# training loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    # do ELBO gradient and accumulate loss\n",
    "    epoch_loss = svi.step(X)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        # report training diagnostics\n",
    "        total_epoch_loss_train = epoch_loss / len(X)\n",
    "        train_elbo[epoch] = total_epoch_loss_train\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for PPCA, we can look at the reconstructions of the inputs $\\textbf{x}$ and see how good they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vae.reconstruct_img(X)\n",
    "reconstruction_mean, reconstruction_var = out\n",
    "reconstruction_mean = reconstruction_mean.cpu().detach().numpy()\n",
    "mae = torch.mean(torch.abs(X-reconstruction_mean))\n",
    "rmse = torch.sqrt(torch.mean((X-reconstruction_mean)**2))\n",
    "mae, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we did for the PPCA model, we can also look at the projections of the data in the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_loc, z_scale = vae.encoder(X)\n",
    "z_loc = z_loc.cpu().detach().numpy()\n",
    "plt.scatter(z_loc[:,0], z_loc[:,1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE application in computer vision\n",
    "\n",
    "The Iris data is relatively simple. Lets now look at a more complex application in computer vision, where the VAE can really start to shine. Concretely, we will try to model images of digits using a VAE - the MNIST dataset. In the code below, we will load the datasets and train the VAE model on it. We will use a larger dimensionality (``z_dim=50``) of the latent space $\\textbf{z}$.\n",
    "\n",
    "Note: training the VAE on this dataset will take a few minutes (about 5-10 minutes, depending on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup MNIST data loaders\n",
    "# train_loader, test_loader\n",
    "train_loader, test_loader = setup_data_loaders(MNIST, use_cuda=use_cuda, batch_size=256)\n",
    "\n",
    "# setup the VAE\n",
    "vae = VAE(use_cuda=use_cuda, z_dim=50, hidden_dim=400, input_dim=784, binary=True)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": 1.0e-3}\n",
    "optimizer = Adam(adam_args)\n",
    "\n",
    "# setup the inference algorithm\n",
    "elbo = Trace_ELBO()\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=elbo)\n",
    "\n",
    "train_elbo = {}\n",
    "test_elbo = {}\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.0\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        # report training diagnostics\n",
    "        normalizer_train = len(train_loader.dataset)\n",
    "        total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "        train_elbo[epoch] = total_epoch_loss_train\n",
    "        print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "        \n",
    "        # initialize loss accumulator\n",
    "        test_loss = 0.0\n",
    "        # compute the loss over the entire test set\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            # if on GPU put mini-batch into CUDA memory\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            # compute ELBO estimate and accumulate loss\n",
    "            test_loss += svi.evaluate_loss(x)\n",
    "\n",
    "        # report test diagnostics\n",
    "        normalizer_test = len(test_loader.dataset)\n",
    "    \n",
    "        total_epoch_loss_test = test_loss / normalizer_test\n",
    "        test_elbo[epoch] = total_epoch_loss_test\n",
    "        print(\"[epoch %03d]  average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
    "        plot_llk(train_elbo, test_elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the reconstructions produced by the VAE model for a subset of the digits in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 10 random test images from the first mini-batch and visualize how well we're reconstructing them\n",
    "fig, axs = plt.subplots(10,2, figsize=(6,20))\n",
    "x = next(iter(test_loader))[0]\n",
    "# if on GPU put mini-batch into CUDA memory\n",
    "if use_cuda:\n",
    "    x = x.cuda()\n",
    "#plot_vae_samples(vae, vis)\n",
    "reco_indices = np.random.randint(0, x.shape[0], 10)\n",
    "for i in range(len(reco_indices)):\n",
    "    index = reco_indices[i]\n",
    "    test_img = x[index, :]\n",
    "    reco_img = vae.reconstruct_img(test_img)\n",
    "    #vis.image(\n",
    "    #    test_img.reshape(28, 28).detach().cpu().numpy(),\n",
    "    #    opts={\"caption\": \"test image\"},\n",
    "    #)\n",
    "    #vis.image(\n",
    "    #    reco_img.reshape(28, 28).detach().cpu().numpy(),\n",
    "    #    opts={\"caption\": \"reconstructed image\"},\n",
    "    #)\n",
    "    axs[i,0].imshow(test_img.reshape(28, 28).detach().cpu().numpy(), cmap=\"Greys\")\n",
    "    axs[i,1].imshow(reco_img.reshape(28, 28).detach().cpu().numpy(), cmap=\"Greys\")\n",
    "    \n",
    "axs[0,0].set_title(\"True image\")\n",
    "axs[0,1].set_title(\"Reconstruction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to visualize the projections of the digits in the latent space. However, in this case, the latent projections are still 50-dimensional!! We will need to project them down even further if we want to visualize them on a 2-D plot. For this, we will use a dimensionality reduction called t-SNE. t-SNE tries to project down the data while attempting to perserve the (Euclidean) distances between all pairs of data points (observations) in the dataset. Lets do that and see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_tsne(vae=vae, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the observations of the different digits are nicely separated (clustered) in the latent space $\\textbf{z}$. I.e., all observations of the digit 5 are clustered together in the latent space, and similarly for other digits.\n",
    "\n",
    "Lastly, since the VAE is a generative model, we can use it to sample new observations (\"fake data\"). We can do this by sampling $z$ from a standard Gaussian distribution and passing those samples through the decoder network. \n",
    "\n",
    "Lets do this to generate 10 new samples of digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    z_loc = torch.zeros(vae.z_dim, dtype=x.dtype, device=x.device)\n",
    "    z_scale = torch.ones(vae.z_dim, dtype=x.dtype, device=x.device)\n",
    "    z = dist.Normal(z_loc, z_scale).rsample()\n",
    "    out = vae.decoder.forward(z)\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(out.reshape(28, 28).detach().cpu().numpy(), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some samples are somewhat realistic images of digits, while others are clearly not. You can probably obtain better samples (i.e., more realistic) if you train the VAE for longer."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "06 - Classification models - Part 4 - Pyro - solutions.ipynb",
   "provenance": [
    {
     "file_id": "1zDyH0gmY7AuI_B2WhQAOynB4OQT0DzLP",
     "timestamp": 1582453187450
    },
    {
     "file_id": "1oDcOaKqMYlOrjxBYZVdYlG1ftELfczp6",
     "timestamp": 1582450341637
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
